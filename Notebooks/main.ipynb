{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/shashankgupta/Documents/code/git_project/plaid_credit\n",
      "/Users/shashankgupta/Documents/code/git_project/plaid_credit/Code\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "cwd = os.getcwd()\n",
    "\n",
    "os.chdir('../')\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "\n",
    "os.chdir('./code/')\n",
    "cwd = os.getcwd()\n",
    "print(cwd)\n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "import seaborn as sns\n",
    "\n",
    "import pickle \n",
    "import logging\n",
    "\n",
    "\n",
    "# from config.config import SQLQuery\n",
    "# querySno = SQLQuery('snowflake')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import Convert,MissingValues,Outlier,FeatureSelection\n",
    "from feature_transformation import Scaler,Transform,Selection\n",
    "from model_building import split_test_train, feature_encoding, classification_models\n",
    "from model_evaluations import model_metrics, feature_importance, probability_bins, cross_validation\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, Normalizer\n",
    "\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "\n",
    "# object initiation \n",
    "tf = Transform()\n",
    "sel = Selection()\n",
    "ft = FeatureSelection()\n",
    "cv = Convert()\n",
    "mv = MissingValues()\n",
    "ot = Outlier()\n",
    "\n",
    "# set seed\n",
    "seed = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_arr = 0.4 # person correlation coefficient # change it to 0.5\n",
    "vif_arr = 5 # vif coefficient\n",
    "features_arr = 10 # total number of features to be selected from backward feature selection\n",
    "iv_upper_limit = 0.5 # upper threshold of iv # change it to 0.6\n",
    "iv_lower_limit = 0.02 # lower threshold of iv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(646, 243)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = pd.read_pickle('/Users/shashankgupta/Documents/code/git_project/plaid_credit/data/final_dataset.pkl')\n",
    "df_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['business_id', 'lending_business_id', 'decision_date', 'drawn_flag',\n",
       "       'everDPD_15', 'fico_score', 'target', 'loans_flag', 'payroll_flag',\n",
       "       'pos_flag',\n",
       "       ...\n",
       "       'sum_credits_grt_1500_6M', 'ratio_credits_lessthan_100_1M_3M',\n",
       "       'ratio_credits_lessthan_100_1M_6M', 'ratio_credits_grt_500_1M_3M',\n",
       "       'ratio_credits_grt_500_1M_6M', 'ratio_credits_grt_1500_1M_3M',\n",
       "       'ratio_credits_grt_1500_1M_6M', 'txn_each_mth_flag',\n",
       "       'txn_grt_100_each_mth_flag', 'txn_flag'],\n",
       "      dtype='object', length=243)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "txn_flag\n",
       "1.0    637\n",
       "0.0      9\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw['txn_flag'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(637, 243)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = df_raw[df_raw['txn_flag']==1]\n",
    "df_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(637, 236)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw = df_raw.drop(['business_id', 'lending_business_id','decision_date','drawn_flag', 'everDPD_15', 'fico_score','txn_flag'], axis=1)\n",
    "df_raw.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train.shape = (445, 235) | y_train.shape = (445,) | x_test.shape = (192, 235) | y_test.shape = (192,)\n"
     ]
    }
   ],
   "source": [
    "## train-test split\n",
    "\n",
    "# train test split\n",
    "x_train, y_train, x_test, y_test = split_test_train(df_raw, target_column='target', test_size=0.3, random_state=seed)\n",
    "print(f'{x_train.shape = }', '|' ,f'{y_train.shape = }', '|' ,f'{x_test.shape = }', '|' ,f'{y_test.shape = }')\n",
    "\n",
    "\n",
    "# copy to df\n",
    "df = x_train.copy(deep=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(445, 235)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get constant features\n",
    "def get_const_features(df):\n",
    "    const_list = []\n",
    "    for col in df.columns: \n",
    "        if (len(df[col].unique())==1):\n",
    "            const_list.append(col)\n",
    "    return(const_list)\n",
    "\n",
    "# remove constant features\n",
    "const_list = get_const_features(df)\n",
    "df = df.drop(columns=const_list)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(445, 226)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get quasi-constant features\n",
    "def get_quasi_const_features(df, threshold=0.01):\n",
    "    qconst_list = []\n",
    "    for col in df.columns: \n",
    "        if (df[col].var() <= threshold):\n",
    "            qconst_list.append(col)\n",
    "    return(qconst_list)\n",
    "\n",
    "# remove constant features\n",
    "qconst_list = get_quasi_const_features(df, threshold=0.01)\n",
    "df = df.drop(columns=qconst_list)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>percent_missing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>stddev_loans_amt_1M</th>\n",
       "      <td>96.179775</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stddev_ecom_amt_1M</th>\n",
       "      <td>93.033708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stddev_loans_amt_3M</th>\n",
       "      <td>91.235955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ratio_stddev_loan_amt_3M_6M</th>\n",
       "      <td>91.235955</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loans_amt_1M</th>\n",
       "      <td>91.011236</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>third_party_flag</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>ecom_flag</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pos_flag</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>txn_each_mth_flag</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>loans_flag</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>226 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                             percent_missing\n",
       "stddev_loans_amt_1M                96.179775\n",
       "stddev_ecom_amt_1M                 93.033708\n",
       "stddev_loans_amt_3M                91.235955\n",
       "ratio_stddev_loan_amt_3M_6M        91.235955\n",
       "loans_amt_1M                       91.011236\n",
       "...                                      ...\n",
       "third_party_flag                    0.000000\n",
       "ecom_flag                           0.000000\n",
       "pos_flag                            0.000000\n",
       "txn_each_mth_flag                   0.000000\n",
       "loans_flag                          0.000000\n",
       "\n",
       "[226 rows x 1 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view missing values\n",
    "def missing_value(df):\n",
    "    percent_missing = df.isnull().sum() * 100 / len(df)\n",
    "    missing_val_df = pd.DataFrame({'percent_missing': percent_missing})\n",
    "    missing_val_df.sort_values(by='percent_missing', ascending=False, inplace=True)\n",
    "    return missing_val_df\n",
    "\n",
    "missing_value(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(445, 187)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = missing_value(df)\n",
    "drop_cols = list(t[t['percent_missing']>80].index)\n",
    "df.drop(drop_cols,axis=1,inplace=True)\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treating missing values\n",
    "df = df.fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'float64': 187}\n",
      "10\n",
      "177\n"
     ]
    }
   ],
   "source": [
    "# get boolean columns\n",
    "def findbool(df):\n",
    "    bool_arr = []\n",
    "    for col in df.columns: \n",
    "        if (len(df[col].unique())<=2):\n",
    "            bool_arr.append(col)\n",
    "    return(bool_arr)\n",
    "\n",
    "# get datatypes frequency\n",
    "def get_datatypes_freq(df):\n",
    "    type_dct = {str(k): list(v) for k, v in df.groupby(df.dtypes, axis=1)}\n",
    "    type_dct_info = {k: len(v) for k, v in type_dct.items()}\n",
    "    return type_dct, type_dct_info\n",
    "\n",
    "type_dct, type_dct_info = get_datatypes_freq(df)\n",
    "print(type_dct_info)\n",
    "\n",
    "bool_col_list = findbool(df)\n",
    "print(len(bool_col_list))\n",
    "\n",
    "type_dct, type_dct_info = get_datatypes_freq(df)\n",
    "col_list = (type_dct['float64'])\n",
    "col_list_excpt_bool = [column for column in col_list if column not in bool_col_list]\n",
    "print(len(col_list_excpt_bool))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## iv_woe\n",
    "\n",
    "def iv_woe(data, target, bins=10, show_woe=False):\n",
    "    \n",
    "    #Empty Dataframe\n",
    "    newDF,woeDF = pd.DataFrame(), pd.DataFrame()\n",
    "    \n",
    "    #Extract Column Names\n",
    "    cols = data.columns\n",
    "    \n",
    "    #Run WOE and IV on all the independent variables\n",
    "    for ivars in cols[~cols.isin([target])]:\n",
    "        if (data[ivars].dtype.kind in 'bifc') and (len(np.unique(data[ivars]))>10):\n",
    "            binned_x = pd.qcut(data[ivars], bins,  duplicates='drop')\n",
    "            d0 = pd.DataFrame({'x': binned_x, 'y': data[target]})\n",
    "        else:\n",
    "            d0 = pd.DataFrame({'x': data[ivars], 'y': data[target]})\n",
    "\n",
    "        \n",
    "        # Calculate the number of events in each group (bin)\n",
    "        d = d0.groupby(\"x\", as_index=False).agg({\"y\": [\"count\", \"sum\"]})\n",
    "        d.columns = ['Cutoff', 'N', 'Events']\n",
    "        \n",
    "        # Calculate % of events in each group.\n",
    "        d['% of Events'] = np.maximum(d['Events'], 0.5) / d['Events'].sum()\n",
    "\n",
    "        # Calculate the non events in each group.\n",
    "        d['Non-Events'] = d['N'] - d['Events']\n",
    "        # Calculate % of non events in each group.\n",
    "        d['% of Non-Events'] = np.maximum(d['Non-Events'], 0.5) / d['Non-Events'].sum()\n",
    "\n",
    "        # Calculate WOE by taking natural log of division of % of non-events and % of events\n",
    "        d['WoE'] = np.log(d['% of Events']/d['% of Non-Events'])\n",
    "        d['IV'] = d['WoE'] * (d['% of Events'] - d['% of Non-Events'])\n",
    "        d.insert(loc=0, column='Variable', value=ivars)\n",
    "        print(\"Information value of \" + ivars + \" is \" + str(round(d['IV'].sum(),6)))\n",
    "        temp =pd.DataFrame({\"Variable\" : [ivars], \"IV\" : [d['IV'].sum()]}, columns = [\"Variable\", \"IV\"])\n",
    "        newDF=pd.concat([newDF,temp], axis=0)\n",
    "        woeDF=pd.concat([woeDF,d], axis=0)\n",
    "\n",
    "        #Show WOE Table\n",
    "        if show_woe == True:\n",
    "            print(d)\n",
    "    return newDF, woeDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information value of total_credit_count_1M is 0.046417\n",
      "Information value of total_credit_count_3M is 0.123842\n",
      "Information value of total_credit_count_6M is 0.106814\n",
      "Information value of total_credit_amt_1M is 0.039476\n",
      "Information value of total_credit_amt_3M is 0.087181\n",
      "Information value of total_credit_amt_6M is 0.046794\n",
      "Information value of credit_size_1M is 0.117904\n",
      "Information value of credit_size_3M is 0.286985\n",
      "Information value of credit_size_6M is 0.244407\n",
      "Information value of stddev_credit_amt_1M is 0.142908\n",
      "Information value of median_credit_amt_1M is 0.14747\n",
      "Information value of stddev_credit_amt_3M is 0.151783\n",
      "Information value of median_credit_amt_3M is 0.195674\n",
      "Information value of stddev_credit_amt_6M is 0.090132\n",
      "Information value of median_credit_amt_6M is 0.254997\n",
      "Information value of ratio_stddev_credit_amt_1M_6M is 0.042563\n",
      "Information value of ratio_stddev_credit_amt_1M_3M is 0.016118\n",
      "Information value of ratio_median_credit_amt_1M_6M is 0.056231\n",
      "Information value of ratio_median_credit_amt_1M_3M is 0.05263\n",
      "Information value of loans_amt_6M is 0.023398\n",
      "Information value of loans_count_6M is 0.034076\n",
      "Information value of median_loans_amt_6M is 0.023398\n",
      "Information value of payroll_amt_6M is 0.024865\n",
      "Information value of payroll_count_6M is 0.005915\n",
      "Information value of median_payroll_amt_6M is 0.024862\n",
      "Information value of stddev_payroll_amt_6M is 0.064408\n",
      "Information value of pos_amt_6M is 0.124942\n",
      "Information value of pos_count_6M is 0.060484\n",
      "Information value of median_pos_amt_6M is 0.165416\n",
      "Information value of stddev_pos_amt_6M is 0.183267\n",
      "Information value of ecom_amt_6M is 0.097237\n",
      "Information value of ecom_count_6M is 0.059462\n",
      "Information value of median_ecom_amt_6M is 0.117647\n",
      "Information value of stddev_ecom_amt_6M is 0.117647\n",
      "Information value of third_party_amt_6M is 0.050978\n",
      "Information value of third_party_count_6M is 0.1205\n",
      "Information value of median_third_party_amt_6M is 0.054732\n",
      "Information value of stddev_third_party_amt_6M is 0.044395\n",
      "Information value of service_amt_6M is 0.031718\n",
      "Information value of service_count_6M is 0.036077\n",
      "Information value of median_service_amt_6M is 0.082993\n",
      "Information value of stddev_service_amt_6M is 0.024148\n",
      "Information value of shops_amt_6M is 0.024596\n",
      "Information value of shops_count_6M is 0.023586\n",
      "Information value of median_shops_amt_6M is 0.072024\n",
      "Information value of stddev_shops_amt_6M is 0.002936\n",
      "Information value of ach_amt_6M is 0.002936\n",
      "Information value of ach_count_6M is 8.1e-05\n",
      "Information value of median_ach_amt_6M is 0.002352\n",
      "Information value of stddev_ach_amt_6M is 0.000419\n",
      "Information value of payroll_amt_3M is 0.02717\n",
      "Information value of payroll_count_3M is 0.031411\n",
      "Information value of median_payroll_amt_3M is 0.015042\n",
      "Information value of stddev_payroll_amt_3M is 0.02457\n",
      "Information value of pos_amt_3M is 0.039683\n",
      "Information value of pos_count_3M is 0.032695\n",
      "Information value of median_pos_amt_3M is 0.128121\n",
      "Information value of stddev_pos_amt_3M is 0.07475\n",
      "Information value of ecom_amt_3M is 0.061982\n",
      "Information value of ecom_count_3M is 0.036288\n",
      "Information value of median_ecom_amt_3M is 0.014495\n",
      "Information value of third_party_amt_3M is 0.050978\n",
      "Information value of third_party_count_3M is 0.092285\n",
      "Information value of median_third_party_amt_3M is 0.088849\n",
      "Information value of stddev_third_party_amt_3M is 0.030494\n",
      "Information value of service_amt_3M is 0.122403\n",
      "Information value of service_count_3M is 0.026231\n",
      "Information value of median_service_amt_3M is 0.047977\n",
      "Information value of stddev_service_amt_3M is 0.002352\n",
      "Information value of shops_amt_3M is 0.08739\n",
      "Information value of shops_count_3M is 0.022842\n",
      "Information value of median_shops_amt_3M is 0.049321\n",
      "Information value of stddev_shops_amt_3M is 0.002352\n",
      "Information value of ach_amt_3M is 0.002936\n",
      "Information value of ach_count_3M is 0.002421\n",
      "Information value of median_ach_amt_3M is 0.000419\n",
      "Information value of payroll_amt_1M is 0.02457\n",
      "Information value of payroll_count_1M is 0.000977\n",
      "Information value of median_payroll_amt_1M is 0.02457\n",
      "Information value of stddev_payroll_amt_1M is 0.000419\n",
      "Information value of pos_amt_1M is 0.052024\n",
      "Information value of pos_count_1M is 0.004532\n",
      "Information value of median_pos_amt_1M is 0.055576\n",
      "Information value of stddev_pos_amt_1M is 0.02457\n",
      "Information value of third_party_amt_1M is 0.034309\n",
      "Information value of third_party_count_1M is 0.039478\n",
      "Information value of median_third_party_amt_1M is 0.098737\n",
      "Information value of stddev_third_party_amt_1M is 0.002936\n",
      "Information value of service_amt_1M is 0.000189\n",
      "Information value of service_count_1M is 0.002148\n",
      "Information value of median_service_amt_1M is 0.002352\n",
      "Information value of shops_amt_1M is 0.000419\n",
      "Information value of shops_count_1M is 1.8e-05\n",
      "Information value of median_shops_amt_1M is 0.000419\n",
      "Information value of payroll_size_1M is 0.054417\n",
      "Information value of pos_size_1M is 0.038154\n",
      "Information value of service_size_1M is 0.007031\n",
      "Information value of shops_size_1M is 0.000189\n",
      "Information value of payroll_size_3M is 0.02717\n",
      "Information value of pos_size_3M is 0.113325\n",
      "Information value of ecom_size_3M is 0.034309\n",
      "Information value of service_size_3M is 0.06234\n",
      "Information value of shops_size_3M is 0.098687\n",
      "Information value of ach_size_3M is 0.002936\n",
      "Information value of loans_size_6M is 0.014495\n",
      "Information value of payroll_size_6M is 0.0589\n",
      "Information value of pos_size_6M is 0.244945\n",
      "Information value of ecom_size_6M is 0.139891\n",
      "Information value of service_size_6M is 0.072024\n",
      "Information value of shops_size_6M is 0.06821\n",
      "Information value of ach_size_6M is 0.000419\n",
      "Information value of ratio_payroll_size_1M_6M is 0.002352\n",
      "Information value of ratio_pos_size_1M_6M is 0.012\n",
      "Information value of ratio_service_size_1M_6M is 0.000419\n",
      "Information value of ratio_shops_size_1M_6M is 0.007653\n",
      "Information value of ratio_payroll_size_1M_3M is 0.000419\n",
      "Information value of ratio_pos_size_1M_3M is 0.012\n",
      "Information value of ratio_service_size__1M_3M is 0.007653\n",
      "Information value of ratio_shops_size_1M_3M is 0.014495\n",
      "Information value of ratio_median_payroll_amt_3M_6M is 0.008256\n",
      "Information value of ratio_median_pos_amt_3M_6M is 0.012\n",
      "Information value of ratio_median_ecom_amt_3M_6M is 0.023398\n",
      "Information value of ratio_median_third_party_amt_3M_6M is 0.079557\n",
      "Information value of ratio_median_service_amt_3M_6M is 0.080985\n",
      "Information value of ratio_median_shops_amt_3M_6M is 0.098687\n",
      "Information value of ratio_median_ach_amt_3M_6M is 0.037826\n",
      "Information value of ratio_stddev_payroll_amt_3M_6M is 0.002352\n",
      "Information value of ratio_stddev_pos_amt_3M_6M is 0.027552\n",
      "Information value of ratio_stddev_third_party_amt_3M_6M is 0.06647\n",
      "Information value of ratio_stddev_service_amt_3M_6M is 0.014495\n",
      "Information value of ratio_stddev_shops_amt_3M_6M is 0.002936\n",
      "Information value of ratio_payroll_size_credit_size_1M is 0.002936\n",
      "Information value of ratio_pos_size_credit_size_1M is 0.020596\n",
      "Information value of ratio_service_size_credit_size_1M is 0.007653\n",
      "Information value of ratio_shops_size_credit_size_1M is 0.007653\n",
      "Information value of ratio_payroll_size_credit_size_3M is 0.017954\n",
      "Information value of ratio_pos_size_credit_size_3M is 0.027265\n",
      "Information value of ratio_ecom_size_credit_size_3M is 0.034309\n",
      "Information value of ratio_service_size_credit_size_3M is 0.065104\n",
      "Information value of ratio_shops_size_credit_size_3M is 0.070538\n",
      "Information value of ratio_ach_size_credit_size_3M is 0.007653\n",
      "Information value of ratio_loans_size_credit_size_6M is 0.023398\n",
      "Information value of ratio_payroll_size_credit_size_6M is 0.082993\n",
      "Information value of ratio_pos_size_credit_size_6M is 0.101512\n",
      "Information value of ratio_ecom_size_credit_size_6M is 0.163957\n",
      "Information value of ratio_service_size_credit_size_6M is 0.051582\n",
      "Information value of ratio_shops_size_credit_size_6M is 0.04727\n",
      "Information value of ratio_ach_size_credit_size_6M is 0.002936\n",
      "Information value of count_credits_lessthan_100_1M is 0.112666\n",
      "Information value of sum_credits_lessthan_100_1M is 0.155921\n",
      "Information value of count_credits_100_to_500_1M is 0.05544\n",
      "Information value of sum_credits_100_to_500_1M is 0.07538\n",
      "Information value of count_credits_grt_500_1M is 0.024608\n",
      "Information value of sum_credits_grt_500_1M is 0.07782\n",
      "Information value of count_credits_grt_1500_1M is 0.047626\n",
      "Information value of sum_credits_grt_1500_1M is 0.062646\n",
      "Information value of count_credits_lessthan_100_3M is 0.13311\n",
      "Information value of sum_credits_lessthan_100_3M is 0.169437\n",
      "Information value of count_credits_100_to_500_3M is 0.125503\n",
      "Information value of sum_credits_100_to_500_3M is 0.098917\n",
      "Information value of count_credits_grt_500_3M is 0.051962\n",
      "Information value of sum_credits_grt_500_3M is 0.126578\n",
      "Information value of count_credits_grt_1500_3M is 0.049676\n",
      "Information value of sum_credits_grt_1500_3M is 0.101139\n",
      "Information value of count_credits_lessthan_100_6M is 0.118544\n",
      "Information value of sum_credits_lessthan_100_6M is 0.18297\n",
      "Information value of count_credits_100_to_500_6M is 0.088252\n",
      "Information value of sum_credits_100_to_500_6M is 0.095597\n",
      "Information value of count_credits_grt_500_6M is 0.063442\n",
      "Information value of sum_credits_grt_500_6M is 0.096474\n",
      "Information value of count_credits_grt_1500_6M is 0.117989\n",
      "Information value of sum_credits_grt_1500_6M is 0.081223\n",
      "Information value of ratio_credits_lessthan_100_1M_3M is 0.023371\n",
      "Information value of ratio_credits_lessthan_100_1M_6M is 0.003004\n",
      "Information value of ratio_credits_grt_500_1M_3M is 0.029344\n",
      "Information value of ratio_credits_grt_1500_1M_3M is 0.029576\n",
      "Information value of ratio_credits_grt_1500_1M_6M is 0.02416\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove features on basis of IV\n",
    "# y_train.reset_index(drop=True, inplace=True)\n",
    "df['target'] = y_train\n",
    "df['target'] = df['target'].astype(float)\n",
    "temp = df.copy()\n",
    "\n",
    "t1, t2 = iv_woe(temp[np.append(col_list_excpt_bool,['target'])], 'target', bins=5, show_woe=False)\n",
    "feature_list = list(t1[ (t1['IV']<iv_upper_limit) & (t1['IV']>iv_lower_limit) ]['Variable'].values)\n",
    "len(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# view correlation\n",
    "corr_df, subset_df = sel.get_correlated_features(df, feature_list, thresh=corr_arr)\n",
    "corr_df\n",
    "\n",
    "\n",
    "# remove correlated features\n",
    "feature_list = sel.corr_iter(df, np.array(feature_list), thresh=corr_arr)\n",
    "feature_list = list(feature_list)\n",
    "len(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get feature list after iterative VIF elimination\n",
    "def vif_iter(df, iv, threshold=10):\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = iv\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(df[iv].values, i) for i in range(len(iv))]\n",
    "    if len(vif_data[vif_data['VIF'] == np.inf]) > 0:\n",
    "        feature = vif_data[vif_data['VIF'] == np.inf]['feature'].iloc[0]\n",
    "        iv.remove(feature)\n",
    "        vif_iter(df, iv, threshold)\n",
    "    elif len(vif_data[vif_data['VIF'] > threshold]) > 0:\n",
    "        feature = vif_data.sort_values(by='VIF', ascending=False)['feature'].iloc[0]\n",
    "        iv.remove(feature)\n",
    "        vif_iter(df, iv, threshold)\n",
    "    vif_data = pd.DataFrame()\n",
    "    vif_data[\"feature\"] = iv\n",
    "    vif_data[\"VIF\"] = [variance_inflation_factor(df[iv].values, i) for i in range(len(iv))]\n",
    "    return iv, vif_data\n",
    "\n",
    "feature_list, vif_df = vif_iter(df, feature_list, threshold=vif_arr)\n",
    "len(feature_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['stddev_credit_amt_3M',\n",
       " 'median_shops_amt_6M',\n",
       " 'stddev_payroll_amt_3M',\n",
       " 'shops_count_3M',\n",
       " 'pos_amt_1M',\n",
       " 'ratio_stddev_third_party_amt_3M_6M',\n",
       " 'ratio_pos_size_credit_size_1M',\n",
       " 'count_credits_lessthan_100_6M',\n",
       " 'count_credits_grt_500_6M',\n",
       " 'ratio_credits_grt_1500_1M_3M']"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Backward feature elimination\n",
    "feat_list = ft.backward_feature_selection(df[feature_list], y_train, num_features=features_arr)\n",
    "feat_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(CVXPY) Jun 29 04:14:24 PM: Encountered unexpected exception importing solver GLOP:\n",
      "RuntimeError('Unrecognized new version of ortools (9.6.2534). Expected < 9.5.0.Please open a feature request on cvxpy to enable support for this version.')\n",
      "(CVXPY) Jun 29 04:14:24 PM: Encountered unexpected exception importing solver PDLP:\n",
      "RuntimeError('Unrecognized new version of ortools (9.6.2534). Expected < 9.5.0.Please open a feature request on cvxpy to enable support for this version.')\n"
     ]
    }
   ],
   "source": [
    "## optimal binning woe\n",
    "\n",
    "import optbinning as optb\n",
    "from optbinning import Scorecard, BinningProcess, OptimalBinning\n",
    "from optbinning.binning.binning_statistics import BinningTable\n",
    "\n",
    "df_temp = df[feat_list].copy()\n",
    "df_temp['target'] = df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "Xt= df_temp['stddev_credit_amt_3M']\n",
    "yt = df_temp['target'].astype(int)\n",
    "\n",
    "optb = OptimalBinning(name='stddev_credit_amt_3M', dtype=\"numerical\", max_n_prebins=4, monotonic_trend='descending',special_codes=[0])\n",
    "optb.fit(Xt, yt)  \n",
    "\n",
    "Xt_binned = optb.transform(Xt)\n",
    "\n",
    "ob_stddev_credit_amt_3M = optb.binning_table.build()\n",
    "\n",
    "optb.binning_table.plot(metric=\"event_rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_stddev_credit_amt_3M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1\n",
    "# Var tranform\n",
    "transformed_vars = x_train[feat_list]\n",
    "\n",
    "# transform\n",
    "col         = 'stddev_credit_amt_3M'\n",
    "conditions  = [ transformed_vars[col] <= 0, \n",
    "                (transformed_vars[col] > 0 ) & (transformed_vars[col] < 578), \n",
    "                (transformed_vars[col] >= 578 ) & (transformed_vars[col] < 1274), \n",
    "                (transformed_vars[col] >= 1274) & (transformed_vars[col] < 7646),\n",
    "                transformed_vars[col] >= 7646 ]\n",
    "\n",
    "choices     = [0.090405,-0.551449,-0.013445, 0.40452,1.951157]\n",
    "    \n",
    "transformed_vars[\"stddev_credit_amt_3M\"] = np.select(conditions, choices, default=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "Xt= df_temp['median_shops_amt_6M']\n",
    "yt = df_temp['target'].astype(int)\n",
    "\n",
    "optb = OptimalBinning(name='median_shops_amt_6M', dtype=\"numerical\", max_n_prebins=4, monotonic_trend='descending',special_codes = [0])\n",
    "optb.fit(Xt, yt)  \n",
    "\n",
    "Xt_binned = optb.transform(Xt)\n",
    "\n",
    "ob_median_shops_amt_6M = optb.binning_table.build()\n",
    "\n",
    "optb.binning_table.plot(metric=\"event_rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_median_shops_amt_6M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2\n",
    "# Var tranform\n",
    "transformed_vars = x_train[feat_list]\n",
    "\n",
    "# transform\n",
    "col         = 'median_shops_amt_6M'\n",
    "conditions  = [ transformed_vars[col] <= 0, \n",
    "                (transformed_vars[col] > 0 ) & (transformed_vars[col] < 109), \n",
    "                (transformed_vars[col] >= 109 ) & (transformed_vars[col] < 954),\n",
    "                transformed_vars[col] >= 954 ]\n",
    "\n",
    "choices     = [0.175146,-0.548348,-0.31506, 0.734762]\n",
    "    \n",
    "transformed_vars[\"median_shops_amt_6M\"] = np.select(conditions, choices, default=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "Xt= df_temp['stddev_payroll_amt_3M']\n",
    "yt = df_temp['target'].astype(int)\n",
    "\n",
    "optb = OptimalBinning(name='stddev_payroll_amt_3M', dtype=\"numerical\", max_n_prebins=4, monotonic_trend='descending',special_codes = [0])\n",
    "optb.fit(Xt, yt)  \n",
    "\n",
    "Xt_binned = optb.transform(Xt)\n",
    "\n",
    "ob_stddev_payroll_amt_3M = optb.binning_table.build()\n",
    "\n",
    "optb.binning_table.plot(metric=\"event_rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_stddev_payroll_amt_3M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "# Var tranform\n",
    "transformed_vars = x_train[feat_list]\n",
    "\n",
    "# transform\n",
    "col         = 'stddev_payroll_amt_3M'\n",
    "conditions  = [ transformed_vars[col] <= 0, \n",
    "                (transformed_vars[col] > 0 ) & (transformed_vars[col] < 901),\n",
    "                transformed_vars[col] >= 901 ]\n",
    "\n",
    "choices     = [0.028132,-0.36385,0.814805]\n",
    "    \n",
    "transformed_vars[\"stddev_payroll_amt_3M\"] = np.select(conditions, choices, default=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4\n",
    "Xt= df_temp['shops_count_3M']\n",
    "yt = df_temp['target'].astype(int)\n",
    "\n",
    "optb = OptimalBinning(name='shops_count_3M', dtype=\"numerical\", max_n_prebins=3, monotonic_trend='descending',special_codes = [0])\n",
    "optb.fit(Xt, yt)  \n",
    "\n",
    "Xt_binned = optb.transform(Xt)\n",
    "\n",
    "ob_shops_count_3M = optb.binning_table.build()\n",
    "\n",
    "optb.binning_table.plot(metric=\"event_rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_shops_count_3M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3\n",
    "# Var tranform\n",
    "transformed_vars = x_train[feat_list]\n",
    "\n",
    "# transform\n",
    "col         = 'shops_count_3M'\n",
    "conditions  = [ transformed_vars[col] <= 0, \n",
    "                (transformed_vars[col] > 0 ) & (transformed_vars[col] < 4.5),\n",
    "                transformed_vars[col] >= 4.5 ]\n",
    "\n",
    "choices     = [0.226839,-0.347537,0.018625]\n",
    "    \n",
    "transformed_vars[\"shops_count_3M\"] = np.select(conditions, choices, default=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5\n",
    "Xt= df_temp['pos_amt_1M']\n",
    "yt = df_temp['target'].astype(int)\n",
    "\n",
    "optb = OptimalBinning(name='pos_amt_1M', dtype=\"numerical\", max_n_prebins=4, monotonic_trend='descending')\n",
    "optb.fit(Xt, yt)  \n",
    "\n",
    "Xt_binned = optb.transform(Xt)\n",
    "\n",
    "ob_pos_amt_1M = optb.binning_table.build()\n",
    "\n",
    "optb.binning_table.plot(metric=\"event_rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_pos_amt_1M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5\n",
    "# Var tranform\n",
    "transformed_vars = x_train[feat_list]\n",
    "\n",
    "# transform\n",
    "col         = 'pos_amt_1M'\n",
    "conditions  = [ transformed_vars[col] < 1780,\n",
    "                transformed_vars[col] >= 1780 ]\n",
    "\n",
    "choices     = [-0.102424,0.586842]\n",
    "    \n",
    "transformed_vars[\"pos_amt_1M\"] = np.select(conditions, choices, default=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6\n",
    "Xt= df_temp['ratio_stddev_third_party_amt_3M_6M']\n",
    "yt = df_temp['target'].astype(int)\n",
    "\n",
    "optb = OptimalBinning(name='ratio_stddev_third_party_amt_3M_6M', dtype=\"numerical\", max_n_prebins=4, monotonic_trend='descending',special_codes = [0])\n",
    "optb.fit(Xt, yt)  \n",
    "\n",
    "Xt_binned = optb.transform(Xt)\n",
    "\n",
    "ob_ratio_stddev_third_party_amt_3M_6M = optb.binning_table.build()\n",
    "\n",
    "optb.binning_table.plot(metric=\"event_rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_ratio_stddev_third_party_amt_3M_6M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6\n",
    "# Var tranform\n",
    "transformed_vars = x_train[feat_list]\n",
    "\n",
    "# transform\n",
    "col         = 'ratio_stddev_third_party_amt_3M_6M'\n",
    "conditions  = [ transformed_vars[col] <= 0,\n",
    "                transformed_vars[col] > 0 ]\n",
    "\n",
    "choices     = [0.290734,-0.271068]\n",
    "    \n",
    "transformed_vars[\"ratio_stddev_third_party_amt_3M_6M\"] = np.select(conditions, choices, default=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7\n",
    "Xt= df_temp['ratio_pos_size_credit_size_1M']\n",
    "yt = df_temp['target'].astype(int)\n",
    "\n",
    "optb = OptimalBinning(name='ratio_pos_size_credit_size_1M', dtype=\"numerical\", max_n_prebins=2, monotonic_trend='descending')\n",
    "optb.fit(Xt, yt)  \n",
    "\n",
    "Xt_binned = optb.transform(Xt)\n",
    "\n",
    "ob_ratio_pos_size_credit_size_1M = optb.binning_table.build()\n",
    "\n",
    "optb.binning_table.plot(metric=\"event_rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_ratio_pos_size_credit_size_1M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7\n",
    "# Var tranform\n",
    "transformed_vars = x_train[feat_list]\n",
    "\n",
    "# transform\n",
    "col         = 'ratio_pos_size_credit_size_1M'\n",
    "conditions  = [ transformed_vars[col] < 0.27,\n",
    "                transformed_vars[col] >= 0.27 ]\n",
    "\n",
    "choices     = [-0.129343,0.326452]\n",
    "    \n",
    "transformed_vars[\"ratio_pos_size_credit_size_1M\"] = np.select(conditions, choices, default=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8\n",
    "Xt= df_temp['count_credits_lessthan_100_6M']\n",
    "yt = df_temp['target'].astype(int)\n",
    "\n",
    "optb = OptimalBinning(name='count_credits_lessthan_100_6M', dtype=\"numerical\", max_n_prebins=2, monotonic_trend='descending',special_codes=[0])\n",
    "optb.fit(Xt, yt)  \n",
    "\n",
    "Xt_binned = optb.transform(Xt)\n",
    "\n",
    "ob_count_credits_lessthan_100_6M = optb.binning_table.build()\n",
    "\n",
    "optb.binning_table.plot(metric=\"event_rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_count_credits_lessthan_100_6M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7\n",
    "# Var tranform\n",
    "transformed_vars = x_train[feat_list]\n",
    "\n",
    "# transform\n",
    "col         = 'count_credits_lessthan_100_6M'\n",
    "conditions  = [ transformed_vars[col] <= 0,\n",
    "                transformed_vars[col] > 0 ]\n",
    "\n",
    "choices     = [0.656801,-0.054845]\n",
    "    \n",
    "transformed_vars[\"count_credits_lessthan_100_6M\"] = np.select(conditions, choices, default=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9\n",
    "Xt= df_temp['count_credits_grt_500_6M']\n",
    "yt = df_temp['target'].astype(int)\n",
    "\n",
    "optb = OptimalBinning(name='count_credits_grt_500_6M', dtype=\"numerical\", max_n_prebins=4, monotonic_trend='descending',special_codes=[0])\n",
    "optb.fit(Xt, yt)  \n",
    "\n",
    "Xt_binned = optb.transform(Xt)\n",
    "\n",
    "ob_count_credits_grt_500_6M = optb.binning_table.build()\n",
    "\n",
    "optb.binning_table.plot(metric=\"event_rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_count_credits_grt_500_6M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9\n",
    "# Var tranform\n",
    "transformed_vars = x_train[feat_list]\n",
    "\n",
    "# transform\n",
    "col         = 'count_credits_grt_500_6M'\n",
    "conditions  = [ transformed_vars[col] <= 0, \n",
    "                (transformed_vars[col] > 0 ) & (transformed_vars[col] < 22.5),\n",
    "                transformed_vars[col] >= 22.5 ]\n",
    "\n",
    "choices     = [-0.57149,-0.129657,0.25218]\n",
    "    \n",
    "transformed_vars[\"count_credits_grt_500_6M\"] = np.select(conditions, choices, default=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10\n",
    "Xt= df_temp['ratio_credits_grt_1500_1M_3M']\n",
    "yt = df_temp['target'].astype(int)\n",
    "\n",
    "optb = OptimalBinning(name='ratio_credits_grt_1500_1M_3M', dtype=\"numerical\", max_n_prebins=5, monotonic_trend='descending',special_codes = [0])\n",
    "optb.fit(Xt, yt)  \n",
    "\n",
    "Xt_binned = optb.transform(Xt)\n",
    "\n",
    "ob_ratio_credits_grt_1500_1M_3M = optb.binning_table.build()\n",
    "\n",
    "optb.binning_table.plot(metric=\"event_rate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ob_ratio_credits_grt_1500_1M_3M"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10\n",
    "# Var tranform\n",
    "transformed_vars = x_train[feat_list]\n",
    "\n",
    "# transform\n",
    "col         = 'ratio_credits_grt_1500_1M_3M'\n",
    "conditions  = [ transformed_vars[col] <= 0, \n",
    "                (transformed_vars[col] > 0 ) & (transformed_vars[col] < 22.5),\n",
    "                transformed_vars[col] >= 22.5 ]\n",
    "\n",
    "choices     = [-0.57149,-0.129657,0.25218]\n",
    "    \n",
    "transformed_vars[\"ratio_credits_grt_1500_1M_3M\"] = np.select(conditions, choices, default=np.nan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparameters\n",
    "params_log_reg = {'penalty': 'l2',\n",
    "                  'random_state': seed,\n",
    "                  'solver': 'liblinear',\n",
    "                  'class_weight': 'balanced'}\n",
    "\n",
    "# model fit\n",
    "logreg_model = classification_models(df[feat_list], y_train, params_log_reg, models=['log_reg'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train cv scores\n",
    "cv_scores = cross_validation(logreg_model, df[feat_list], y_train, scoring='roc_auc', folds=3, seed=seed)\n",
    "print('CV Scores -',np.round(cv_scores, 2))\n",
    "print('Mean of CV Scores -',np.round(np.mean(cv_scores),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance\n",
    "feat_imp = feature_importance(logreg_model, df[feat_list], show_plot=True)\n",
    "\n",
    "feat_imp.sort_values(by='importance', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Test model\n",
    "\n",
    "\n",
    "# reset index\n",
    "# x_test.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# binning variable transform\n",
    "x_test = x_test[feat_list]\n",
    "x_test.fillna(0, inplace=True)\n",
    "x_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_test.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test cv scores\n",
    "cv_scores = cross_validation(logreg_model, x_test[feat_list], y_test, scoring='roc_auc', folds=3, seed=seed)\n",
    "print('CV Scores -',np.round(cv_scores, 2))\n",
    "print('Mean of CV Scores -',np.round(np.mean(cv_scores),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Model Evaluation - KS & ROC AUC\n",
    "\n",
    "def ks(target=None, prob=None):\n",
    "    data = pd.DataFrame()\n",
    "    data['y'] = target\n",
    "    data['y'] = data['y'].astype(float)\n",
    "    data['p'] = prob\n",
    "    data['y0'] = 1- data['y']\n",
    "    data['bucket'] = pd.qcut(data['p'], 5)\n",
    "    grouped = data.groupby('bucket', as_index=False)\n",
    "    kstable = pd.DataFrame()\n",
    "    kstable['min_prob'] = grouped.min()['p']\n",
    "    kstable['max_prob'] = grouped.max()['p']\n",
    "    kstable['events'] = grouped.sum()['y']\n",
    "    kstable['nonevents'] = grouped.sum()['y0']\n",
    "    kstable = kstable.sort_values(by='min_prob', ascending=False).reset_index(drop=True)\n",
    "    kstable['event_rate'] = (kstable.events / data['y'].sum()).apply('{0:.2%}'.format)\n",
    "    kstable['nonevent_rate'] = (kstable['nonevents'] /  data['y0'].sum()).apply('{0:2%}'.format)\n",
    "    kstable['cum_eventrate'] = (kstable.events / data['y'].sum()).cumsum()\n",
    "    kstable['cum_noneventrate'] = (kstable.nonevents / data['y0'].sum()).cumsum()\n",
    "    kstable['KS'] = np.round(kstable['cum_eventrate'] - kstable['cum_noneventrate'], 3) * 100\n",
    "    kstable['bad_rate'] = (kstable['events'] / (kstable['events'] + kstable['nonevents'])) * 100\n",
    "    \n",
    "    # formatting\n",
    "    kstable['cum_eventrate'] = kstable['cum_eventrate'].apply('{0:.2%}'.format)\n",
    "    kstable['cum_noneventrate'] = kstable['cum_noneventrate'].apply('{0:.2%}'.format)\n",
    "    kstable.index = range(1,6)\n",
    "    kstable.index.rename('Decile', inplace=True)\n",
    "    pd.set_option('display.max_columns', 9)\n",
    "    print(kstable)\n",
    "    \n",
    "    # Display KS\n",
    "    print(\"KS is \" + str(max(kstable['KS']))+\"%\"+ \" at decile \" + str((kstable.index[kstable['KS']==max(kstable['KS'])][0])))\n",
    "    return kstable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predicted proability\n",
    "train_pred = logreg_model.predict_proba(df[feat_list])[:,1]\n",
    "                                                                     \n",
    "test_pred = logreg_model.predict_proba(x_test[feat_list])[:,1]\n",
    "\n",
    "\n",
    "train_ks = ks(y_train, train_pred)\n",
    "test_ks = ks(y_test, test_pred)\n",
    "\n",
    "from sklearn.metrics import  roc_auc_score\n",
    "\n",
    "print(roc_auc_score(y_train, train_pred))   \n",
    "  \n",
    "print(roc_auc_score(y_test, test_pred))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.express as px\n",
    "# copy df\n",
    "df_all = df_raw.copy()\n",
    "\n",
    "# reset index\n",
    "df_all.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# treat missing values\n",
    "df_all = df_all.fillna(0)\n",
    "\n",
    "predicted_probas = logreg_model.predict_proba(df_all[feat_list])\n",
    "df_all['proba'] = predicted_probas[:,1:].flatten()\n",
    "px.histogram(df_all['proba'], nbins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train\n",
    "x_train = df.copy()\n",
    "x_train.reset_index(drop=True,inplace=True)\n",
    "y_train.reset_index(drop=True,inplace=True)\n",
    "\n",
    "df_train = x_train.copy()\n",
    "df_train['target'] = y_train\n",
    "df_train['proba'] = logreg_model.predict_proba(x_train[feat_list])[:,1:].flatten()\n",
    "\n",
    "df_train['proba'] = np.round(df_train['proba'], 3)\n",
    "df_train['DecileRank']= pd.qcut(df_train['proba'], q = 4)\n",
    "df_stats = pd.DataFrame(np.round(df_train.groupby(by='DecileRank')['target'].mean(),3))\n",
    "df_stats['volume'] = df_train.groupby(by='DecileRank')['target'].count()\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test\n",
    "x_test.reset_index(drop=True,inplace=True)\n",
    "y_test.reset_index(drop=True,inplace=True)\n",
    "\n",
    "df_test = x_test.copy()\n",
    "df_test['target'] = y_test\n",
    "df_test['proba'] = logreg_model.predict_proba(x_test[feat_list])[:,1:].flatten()\n",
    "\n",
    "df_test['proba'] = np.round(df_test['proba'], 3)\n",
    "df_test['DecileRank']= pd.qcut(df_test['proba'], q = 4)\n",
    "df_stats = pd.DataFrame(df_test.groupby(by='DecileRank')['target'].mean())\n",
    "df_stats['volume'] = df_test.groupby(by='DecileRank')['target'].count()\n",
    "df_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.17 64-bit ('venv_plaid_credit')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d9bbc33ec67b6e69663d9524a7cba3ebe2e2de2798633e20e7d227e00dd346ad"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
